"""
Medical image semantic segmentation API functions. Orchestrates the tasks of
- App logging
- App reporting
- Dataset generation, processing
- Model interfacing: Run in Train, Inference, Cross-validation and PArameter Optimization modes.
"""

import os
from pathlib import Path
import json

from typing import Optional
from datetime import datetime

from abc import ABC, abstractmethod
from ..dataset.dataset_api import DatasetEngine


class APIMethod(ABC):
    def __init__(self,
            logger,
            config=None,
            cubic: bool = False,
            reporting_target_folder: str = None,
            dataset_objects: dict = None

        ):
        """

        :param reporting_target_folder: Path to which the following files are stored:
            - model checkpoints when in Train mode.
            - Model evaluation when in Inference mode.
            - a subfolder named "cross_validation_run" is created and stores all Folds models.
            This folder is generated by the parent APIMethod or passed here (for CV run mode).
        """
        self.logger = logger
        self.config = config
        self.reporting_target_folder = self.config.get("reporting_target_folder")
        self.inference_dataset_key = self.config.get("inference_dataset_key")
        self.dataset_objects = dataset_objects

    def generate_report(self):
        """
        Generate model app reporting, and folders.
        If one is passed via configuration, then that becomes the main reporting folder.
        (example: from a previous model Train run.)
        """

        def make_folder_try(
            folder_path,
            error_message
        ):
            """
            Make dir helper fuction.
            """

            try:
                os.mkdir(folder_path)
            except:
                raise error_message

        if self.config.get("generate_report_folder") and 'LOCAL_RANK' not in os.environ.keys() and 'NODE_RANK' not in os.environ.keys():

            # Generate main report folder
            self.reporting_target_folder = os.path.join(
                self.config.get("reporting_target_folder"), f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )

            make_folder_try(
                self.reporting_target_folder,
                f"Error generating reporting path reporting_target_folder {self.reporting_target_folder}"
            )

            # Generate the dataset_repository
            make_folder_try(
                os.path.join(self.reporting_target_folder, "dataset"),
                f"Error generating reporting path reporting_target_folder {self.reporting_target_folder}"
            )

            # Generate the test_dataset_results
            make_folder_try(
                os.path.join(self.reporting_target_folder, "test_dataset_results"),
                f"Error generating reporting path reporting_target_folder {self.reporting_target_folder}"
            )

            self.logger.debug(f"Generated report folder tree at: {self.reporting_target_folder}")

        else:
            self.logger.debug(f"Using existing report folder: {self.reporting_target_folder}")

        # Convert and write JSON object to file
        with open(os.path.join(
                self.reporting_target_folder,
                "app_config.json"
        ), "w") as outfile:
            json.dump(self.config.get_config(), outfile)


    def call(
        self,
        dataset_objects: Optional[dict] = None
    ):
        pass


    def go(
        self,
        custom_run: bool = False
    ):
        """
        :param custom_run: When running isolated Model train or inference using purely config
            file dataset specifications, this flag should be off, so dataset objects are generation
            upon initialization. A Custom run, in case of cross validation, or parameter optimization
            should have this flag True.
        """
        try:
            self.generate_report()
            if self.dataset_objects is None:
                train_dataset_objects, val_dataset_objects = self.validade_datasets()

            # if custom_run:
            #     return self.call()
            # else:
            #     if self.dataset_objects is None:
            #         dataset_objects = self.generate_datasets()
            #     else:
            #         dataset_objects = self.dataset_objects

            return self.call(train_dataset_objects, val_dataset_objects)

        except:
            # self.logger.exception("Execution Error")
            raise


    def validade_datasets(self):
        """
        Method run to validate produced datasets.
        """
        return DatasetEngine(
            logger=self.logger,
            config=self.config,
        ).go()

    def validade_algorithm(self):
        pass